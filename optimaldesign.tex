\documentclass[12pt]{iopart}
\usepackage{graphicx}
\usepackage{iopams}
\begin{document}
\article[Optimal design for spectroscopy]{TECHNICAL DESIGN NOTE}{Optimal design for spectroscopy}
%%\author{G Imreh and W-Y Cheng}
\address{Institute of Atomic and Molecular Sciences, Academia Sinica, Taiwan}
%%\ead{\mailto{imrehg@gmail.com}}
\begin{abstract}
Optimal design methodology for spectroscopy and other fitted things.
\end{abstract}

\noindent{\it Keywords\/}:

\pacs{}
%%\submitto{\MST}

\section{Introduction}

Experiments are not limited to the physical sciences, but many areas of science, even industry makes use of them: a process is described by a mathematical model and repeated observations are used to evaluate the model and estimate its free parameters. When designing an experiment, the most common aims are the ability to estimate the model parameters without bias and as well as possible, ability to detect lack of fit, and to use the fewest number of observations\cite{Box1975,Box1987}. The last aim is often considered secondary in physical experiments, however as this note will show, the three aims are interconnected. 

The possible input parameters for an experiment form the $\mathcal{X}$ design region. From $\mathcal{X}$ one chooses $N$ points $x_i$, $i = 1, \ldots, N$ to conduct the experiment. Using optimal experimental design (OED) theory, with different choices of $x_i$ one can adjust the experiment towards different goals: overall best estimate of the model parameters, best estimate (smallest variance) of a subset of important parameters at the expence of others, best estimate of a function of the model parameters, most sensitive lack-of-fit measure. The list is by no means exhausive, but represent the most common cases in physics experiments. Most researchers spread the observations uniformly in the region of interest, which is often sub-optimal in all criteria. OED is a rich area of statistics, mostly used in medical research and industrial quality control, mainly due to those fields close connection to statistics, not because of the lack relevance to other fields. There were previous efforts to use OED for physics, such as optimal measurement of processes described by differential equations, e.g. the placement of temperature sensors in heat conductivity experiments\cite{Emery1998}. This note hopes to introduce the methodology of OED for model parameter estimation (fitting), examine its suitability for physical experiments and give examples to help implementation.


\section{Optimal design}
Short introduction of linear regression using matrix algebra, only to show the notation in this paper. For more detailed explanation check any standard linear algebra text book. Measure $y_i$ at $x_i$ points ($i=1, \ldots, N$)
\begin{equation}
E[Y] = F \beta 
\end{equation}
where $Y$ is the $N \times 1$ vector of responses, $\beta$ is a $p \times 1$ vector of unknown parameters, and $F$ is the $N \times p$ extended design matrix. The $i$th row of $F$ is $f^T(x_i)$, a known function of the $m$ explanatory variables.

Least squares regression is then:
\begin{equation}
\hat \beta = (F^T F)^{-1} F^T y
\end{equation}
The covariance matrix of the least squares estimates is
\begin{equation}
\mathrm{var} \hat \beta = \sigma^2 (F^T F)^{-1}
\end{equation}


\subsection{Obtimal design}
Over $\xi$ is a continuous measure over $\mathcal{X}$. If $\xi$ has $n$ distinct points, then
\begin{equation}
\xi = \left\{ 
  \begin{array}{l l l l}
    x_1 & x_2 & \ldots & x_n\\
    w_1 & w_2 & \ldots & w_n\\
  \end{array} \right \}
\end{equation}
where $w_i$ is the weight of point $x_i$, $\int_{\mathcal{X}}\xi(dx) = 1$ and $0 \leq w_i \leq 1$ for every $i$.

Exact design is for realizable integers for a given $N$.
\begin{equation}
\xi_N = \left\{ 
  \begin{array}{l l l l}
    x_1 & x_2 & \ldots & x_n\\
    r_1/N & r_2/N & \ldots & r_n/N\\
  \end{array} \right \}
\end{equation}

The N-trial design:
\begin{equation}
F^T F = \sum_{i=1}^N f(x_i) f(x_i)^T
\end{equation}.
The information matrix is
\begin{equation}
M(\xi) = \int_{\mathcal{X}} f(x)f^T(x) \xi(dx) = \sum_{i=1}^n f(x_i)f^T(x_i)w_i
\label{eq:infcont}
\end{equation}
summing over the $n$ design points.
For the exact design we have to scale as
\begin{equation}
M(\xi_N) = \frac{F^T F}{N}
\end{equation}
The variance of the response is given by
\begin{equation}
\mathrm{var}\{\hat y(x)\} = \sigma^2 f^T(x) M^{-1}(\xi) f(x)
\end{equation}
from where the standardized variance of the predicted response is
\begin{equation}
d(x, \xi) = f^T(x) M^{-1}(\xi) f(x)
\end{equation}
that is function of both $x$ where the prediction is made and the design $\xi$.

D-optimal design (as "determinant"): $|F^T F|$ is maximal.
One practical testing method whether one found the D-optimal design: for D-optimality, $d(x, \xi) \leq p$, with equality at the design points.
Works for any differentiable functions, even if only differentiable numerically.

If $\mathcal{X} = \mathbb{R}$ then $\xi$ has $p$ support points, and all of them have equal weight. If $\mathcal{X} \subsetneq \mathbb{R}$ then $n \geq p$, and might have unequal weights.

Talk about the effect of singular matrices.

There is no standard way to caulculate the optimal $\xi$ in general. There are methods developed (see citation, ...). One subset of methods is sequential design: adding points one-by-one, or removing points one-by-one. One practical method is outlined here:

\begin{enumerate}
\item Nonsingular start with $n$ points
\item Calculate argmax $d(x, \xi_n)$ for $x \in \mathcal{X}$,
\item Add that point to the bunch as $n+1$
\item Repeat until algorithm converges
\item Can throw away starting points
\item Should have an approximate measure of the optimal $\xi^*$
\end{enumerate}
Works for any differentiable functions, even if only differentiable numerically.

\subsection{Restricted optimal design}

\begin{equation}
d_s(x, \xi) = f^T(x) M^{-1}(\xi)f(x) - f_2^T(x) M_{22}^{-1}(\xi)f_2(x)
\label{eq:dsvar}
\end{equation}

\subsection{Sub-obtimal design for robustness}

Surprisingly robust, the $\hat \beta$ has to be very far from the true to lose power... Do simulation!

\subsection{Finite sample size}

The relative D-efficiency of two models $\xi_1$ and $x_2$ is defined as 
\begin{equation}
D_\mathrm{eff} = \left\{\frac{M(\xi_1)}{M(\xi_2)}\right\}^{1/p}
\end{equation}
which results in an efficiency measure which is proportional to the design size, irrespective to the dimension of the model. Thus e.g. $D_\mathrm{eff} = 0.5$ means that $1 / D_\mathrm{eff} = 2$ replicates of design measure $\xi_1$ would be as efficient as one replicate of $\xi_2$.

In this paper, thus we can set $\xi_1$ to the uniform distribution of measurement points, and compare optimized versions of $\xi_2$ to that.

\subsection{Design for restricted parameter space}

Doing the same method for restricting $\mathcal{X}$ is just as possible. 

\subsection{Heterosadicity solved by GLM}
\subsection{Model validation}

Maximize the lack-of fit parameter: T-optimization (as "test"). If there are $p_1$ and $p_2$ 

\subsection{Imprecision of $\hat \beta$}

How to calculate the predicted effect of imprecise parameters?

\subsection{Functions of variables}

C-optimum design (as "constans"):

\begin{equation}
\mathrm{var}\left\{g(\hat \theta)\right\} = \mathrm{var}(c^T \hat \beta) = c^T M^{-1}(\xi) c
\end{equation}
where $c$ is a $p \times 1$ vector of known constans. Generally, it can be defined from the Tailor expansion:
\begin{equation}
c_i(\theta) = \frac{\partial g(\theta)}{\partial \theta_i}
\end{equation}

Have to be careful, though, because the model can be singular. For linear models this means maybe only linear combinations of parameters can be estimated, for nonlinear models it is possible that the parameter won't be observable. Care needed!

\section{Example: Lorentzian spectral line}

The following examples of optimal design are based on spectroscopic experiments, but should be general enough to provide guidance for any other models.

In this hypothetical experiment we are trying to measure an atomic fluorescence lineshape of simple system, a Lorentzian function of the excitation light frequency:
\begin{equation}
\eta(x) = I \frac{\Gamma}{(x - x_0)^2 + \Gamma^2}
\end{equation}
where $I$ is the height of the peak, $x_0$ is the centre frequency and $\Gamma$ is the half-with at half maximum. The partial derivatives for the Taylor-expanson are
\begin{eqnarray}
\frac{\partial \eta(x)}{\partial x_0} &=& I \frac{2 \Gamma (x - x_0)}{\left[(x - x_0)^2 + \Gamma^2\right]^2} \\
\frac{\partial \eta(x)}{\partial \Gamma} &=& I \frac{(x - x_0)^2 - \Gamma^2}{\left[(x - x_0)^2 + \Gamma^2\right]^2} \\
\frac{\partial \eta(x)}{\partial I} &=& \frac{\Gamma}{(x - x_0)^2 + \Gamma^2}
\end{eqnarray}
from which, our linearized model will be
\begin{equation}
E[y] = \eta(x) + (x_0 - \hat x_0) \frac{\partial \eta(x)}{\partial x_0} + (\Gamma - \hat \Gamma) \frac{\partial \eta(x)}{\partial \Gamma} + (I - \hat I) \frac{\partial \eta(x)}{\partial I}
\end{equation}
that can be reorganized as
\begin{eqnarray}
E[y] - \eta(x) &=& (x_0 - \hat x_0) \frac{\partial \eta(x)}{\partial x_0} + (\Gamma - \hat \Gamma) \frac{\partial \eta(x)}{\partial \Gamma} + (I - \hat I) \frac{\partial \eta(x)}{\partial I}  \nonumber \\
E[Y] &=& \beta F
\end{eqnarray}
where the coefficients are
\begin{equation}
\beta = \left[ 
  \begin{array}{l}
    x_0 - \hat x_0 \\
    \Gamma - \hat \Gamma\\
    I - \hat I \\
  \end{array} \right ] 
\end{equation}
 and the $i$th row of $F$ is
\begin{equation}
 f^T(x_i) = \left[ 
  \begin{array}{l l l}
   \frac{\partial \eta(x_i)}{\partial x_0} & \frac{\partial \eta(x_i)}{\partial \Gamma} & \frac{\partial \eta(x_i)}{\partial I}
  \end{array} \right ]
\end{equation}
Without loss of generalization we can set $\hat x_0 = 0$, $\hat \Gamma = 1$, $\hat I = 1$, since the $x$ and $y$ range can always be rescaled to these values. Then we can look for the optimal design in the form of 
\begin{equation}
\xi = \left\{ 
  \begin{array}{c c c}
    -a & 0 & a\\
    \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
  \end{array} \right \}
\end{equation}
since our original function is symmetric with respect to 0 (hence $-a$ and $a$) and $\mathcal{X}~=~\mathbb{R}$ (thus the number of design points equal the dimension of $\beta$, with equal weights). Under these considetations we can calculate the information matrix $M$ as in \eref{eq:infcont}. The maximum of the determinat is found by differentiating with respect to $a$ and finding the roots. The calculation is straightforward but lengthy, thus we only quote the result of $a~\approx~0.7746$. This analytical method can cumbersome except for the simplest models (even with the available symbolic mathematic software). On the other hand, the sequential method of finding the correct parameters is much less intensive computationally. Let's start with a design of $x = [-1, 0, 1]$ which is still symetric and equal probability, but this is just to speed up the convergence of the algorithm. Any other $n \geq 3$ number for which $M$ is not singular could be chosen as a starting point. Then the sequential algorithms will create the following set of design points:
\begin{equation}
    -1, 0, 1, 0.5772, -0.5772, 0, -0.7886,\ldots,0,-0.7746,0.7746,\ldots
\end{equation}
where the final ``$\ldots$'' just repeats the $0,-0.7746,0.7746$ sequence. We can then ignore the starting values and conclude that are design should be
\begin{equation}
\xi = \left\{ 
  \begin{array}{c c c}
    -0.7746 & 0 & 0.7746\\
    \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
  \end{array} \right \}
\end{equation}
In the case when $\mathcal{X}$ is restricted it is possible that we have more than $p$ number of support for our final design and they won't have equal weights, thus a frequency analysis of the output of the sequential design is needed.



\section{What have we learned?}

\ack Thanks for the dudes and to the National Science Council of Taiwan.

\section*{References}
\bibliographystyle{unsrt}
\bibliography{optimal}
\end{document}
