\documentclass[12pt]{iopart}
\usepackage{iopams}
\usepackage{graphicx}
\usepackage{enumerate}
\begin{document}
\article[Optimal design methods for fitting experimental data]{TECHNICAL DESIGN NOTE}{Optimal design methods for fitting experimental data}
%%\author{G Imreh and W-Y Cheng}
\address{Institute of Atomic and Molecular Sciences, Academia Sinica, Taiwan}
%%\ead{\mailto{imrehg@gmail.com}}
\begin{abstract}
Optimal design methodology for spectroscopy and other fitted things.
\end{abstract}

\noindent{\it Keywords\/}: optimal experiment design, regression

\pacs{}
%%\submitto{\MST}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Theory

\section{Introduction}


Experiments are essential to science. In the physical sciences most commonly a process is described by a mathematical model and repeated observations are used to evaluate the model and estimate its free parameters. When designing an experiment, the most common aims are the ability to estimate the model parameters without bias and as well as possible, ability to detect lack of fit, and to use the fewest number of observations \cite{Box1975,Box1987}. The last aim is often considered secondary in physical experiments, however as this note will show, the three aims are interconnected.

The possible input parameters for an experiment form the $\mathcal{X}$ design region. From $\mathcal{X}$ one chooses $N$ points $x_i$, $i = 1, \ldots, N$ to conduct the experiment. Using optimal experimental design (OED) theory, with different choices of $x_i$ one can adjust the outcome towards different goals, e.g. overall best estimate of the model parameters, best estimate (smallest variance) of a subset of important parameters at the expense of others, most sensitive lack-of-fit measure. The list is by no means exhaustive, but represent the common cases in physics. Most researchers spread the observations uniformly in the region of interest, which is often not the most powerful method to achieve any of these goals. OED is a rich area of statistics, so far mostly used in medical research and industrial quality control, mainly due to those fields close connection to statistics, not because of the lack relevance to other fields. There were previous efforts to use OED for physics, such as optimal measurement of processes described by differential equations, e.g. the placement of temperature sensors in heat conductivity experiments \cite{Emery1998}. This note hopes to introduce the methodology of OED for model parameter estimation (fitting), examine its suitability for physical experiments and give examples to help implementation.


\section{Optimal design}
\label{sec:optdesign}
Optimal design is based on ideas from linear regression. A short introduction of linear regression analysis with matrix algebra is given in this section to introduce the notation used in this note. For more detailed explanation check any standard linear algebra text book. The measurement of a physical parameter $y_i$ with input $x_i$ ($i=1, \ldots, N$) has an expectation value
\begin{equation}
E[Y] = F \beta
\end{equation}
where $Y$ is the $N \times 1$ vector of responses, $\beta$ is a $p \times 1$ vector of unknown parameters, and $F$ is the $N \times p$ extended design matrix. The $i$th row of $F$ is $f^T(x_i)$, a known function of the $m$ explanatory variables. As an example, for the model $y = \beta_1 + \beta_2 x_1 + \beta_3 x_1 x_2 + \beta_4 x_2^2$ the information matrix has rows of
\begin{equation}
f^T(x_i) = \left[
	\begin{array}{c c c c}
	1 & x_{1i} & x_{1i}x_{2i} & x_{2i}^2.
	\end{array}\right]
\end{equation}
For this model $m = 2$ and $p = 4$. The least squares linear regression is then written as:
\begin{equation}
\hat \beta = (F^T F)^{-1} F^T y
\end{equation}
and the variance matrix of the least squares estimates is
\begin{equation}
\mathrm{var} \hat \beta = \sigma^2 (F^T F)^{-1}
\label{eq:varbeta}
\end{equation}
where $\sigma$ is the error variance. The errors are assumed to be normally distributed with 0 mean. The predicted value of the function is
\begin{equation}
\hat y(x) = f^T(x) \hat \beta
\end{equation}
which have variance
\begin{equation}
\mathrm{var}\{\hat y(x)\} = \sigma^2 f^T(x)\left(F^T F\right)^{-1}f(x).
\end{equation}
Optimal design then stems from the question: which are the best $x_i$ points to measure so that the variance of the model parameters is minimized? The answer to this question also points the the way to optimize $x_i$ for other goals.

Without a loss of generality we can assume the explanatory variable $x$ as one-dimensional. 
Let's define the continuous design function $\xi$ as a measure over the design region $\mathcal{X}$. If $\xi$ has $n$ distinct support points, then
\begin{equation}
\xi = \left\{
  \begin{array}{l l l l}
    x_1 & x_2 & \ldots & x_n\\
    w_1 & w_2 & \ldots & w_n\\
  \end{array} \right \}
\label{eq:xicont}
\end{equation}
where $w_i$ is the weight of point $x_i$, $\int_{\mathcal{X}}\xi(dx) = 1$ and $0 \leq w_i \leq 1$ for every $i$. In our problem setting a specific $\xi$ means to conduct the experiment with $x_i$ input parameters in $w_i$ fraction of the time.

For clarity, $\xi$ can also be rewritten with delta-functions as
\begin{equation}
\xi(x) = \sum_i w_i \delta(x - x_i).
\label{eq:xidelta}
\end{equation}

Exact design is finite version of $\xi$ for realizable integers $r_i$ for a total number of $N = \sum_i r_i$ trials:
\begin{equation}
\xi_N = \left\{
  \begin{array}{l l l l}
    x_1 & x_2 & \ldots & x_n\\
    r_1/N & r_2/N & \ldots & r_n/N\\
  \end{array} \right \}
\label{eq:xiexact}
\end{equation}
In the limit of $N \rightarrow \infty$ this means $\xi_N \rightarrow \xi$.
The information matrix of a model given $F$ will depend on the design function as
\begin{equation}
M(\xi) = \int_{\mathcal{X}} f(x)f^T(x) \xi(dx) = \sum_{i=1}^n f(x_i)f^T(x_i)w_i
\label{eq:infcont}
\end{equation}
summing over the $n$ design points.
The N-trial exact design is
\begin{equation}
F^T F = \sum_{i=1}^N f(x_i) f(x_i)^T
\end{equation}
which has to be scaled to get the information matrix of
\begin{equation}
M(\xi_N) = \frac{F^T F}{N}
\end{equation}
The variance of the response is then given by
\begin{equation}
\mathrm{var}\{\hat y(x)\} = \sigma^2 f^T(x) M^{-1}(\xi) f(x).
\end{equation}
To get a variance measure that is independent of the number of trials and the (usually unknown) variance of errors, the standardized variance is
\begin{equation}
d(x, \xi) = f^T(x) M^{-1}(\xi) f(x)
\label{eq:stdvar}
\end{equation}
which is function of both $x$ where the prediction is made and the design $\xi$.
For an exact design
\begin{equation}
d(x, \xi_N) = f^T(x) M^{-1}(\xi_N) f(x) = \frac{N \mathrm{var}\{\hat y(x)\}}{\sigma^2}
\end{equation}
Looking at the variance of the fitted model parameters as shown in \eref{eq:varbeta}, it depends on the inverse of the information matrix $M = F^TF$. The best prediction is achieved when $M^{-1}$ is minimal (or equivalently when $M$ is maximal). In general $\beta$ is a vector, thus one wants to maximize the determinant $|M|$ for the overal best prediction. This is the so--called $D$-optimal design (as ``determinant'').

\subsection{General Equivalence Theorem}
\label{sec:get}
The General Equivalence Theorem is a versatile too find optimal designs under different optimality conditions. In the theory of continuous designs one considers the minimization of a general function of imprecision $\Psi\{M(\xi)\}$. If $\mathcal{X}$ is compact, and $\Psi$ convex and differentiable, it can be shown that $\xi$ that minimizes $\Psi$ also satisfies a number of other conditions. 
The General Equivalence Theorem is then an application of the fact that the derivatives of a function are zero at its minimum. In our case $\Psi$ depends on the measure $\xi$ through the information matrix. Let $\bar \xi$ put unit mass at a point $x$ and a new measure $\xi'$ be given by
\begin{equation}
\xi' = (1-\alpha)\xi + \alpha \bar \xi
\end{equation}
\Eref{eq:infcont} shows that the information matrix is additive, thus
\begin{equation}
M(\xi') = (1-\alpha)M(\xi) + \alpha M(\bar \xi).
\end{equation}
The derivative of $\Psi$ is then
\begin{equation}
\phi(x, \xi) = \lim_{\alpha \rightarrow 0^+} \frac{1}{\alpha}\left[\Psi\{(1-\alpha)M(\xi) + \alpha M(\bar \xi)\} - \Psi\{M(\xi)\}\right]
\label{eq:deriv}
\end{equation}
The General Equivalence Theorem then states that the conditions
\begin{enumerate}[(a)]
\item the design $\xi^*$ minimizes $\Psi\{M(\xi)\}$
\item the minimum of $\phi(x, \xi^*) \geq 0$
\item $\phi(x, \xi^*)$ achieves minimum at the support points of the design
\end{enumerate}
are equivalent. 

For example in the case of $D$-optimality the determinant of the information matrix is maximized, thus the imprecision can be defined as
\begin{equation}
\Psi\{M(\xi)\} = \log|M^{-1}(\xi)| = - \log|M(\xi)|.
\end{equation}
The derivative function \eref{eq:deriv} then can be shown to be
\begin{equation}
\phi(x, \xi) = p - d(x, \xi)
\label{eq:phicond}
\end{equation}
where $p$ is the number of parameters of the model and $d(x, \xi)$ is the standardized variance defined in \eref{eq:stdvar}. From condition (a) above, the optimal design $\xi^*$ will be 
\begin{equation}
d(x, \xi^*) \leq p
\end{equation}
for $x \in \mathcal{X}$ with equivalence at the support points.

\subsection{Finding the optimal design}
\label{sec:findoptimal}
The General Equivalence Theorem implies that away from the optimum $\phi(x, \xi) < 0$. Then we can choose the measure $\bar \xi_k$ that puts unit mass at point $x_k$ as $\phi(x_k, \xi_k) < 0$, then
\begin{equation}
\xi_{k+1} = (1-\alpha_k) \xi_k + \alpha \bar \xi_k
\end{equation}
will lead to decrease in $\Psi$ if $\alpha_k$ is small enough. The above describes a family of descent algorithms. The best convergence can be achieved if $x_k$ chosen as $\phi(x_k, \xi_k)$ has a minimum. In the original construction of these algorithms $\alpha_k = (k+1)^{-1}$ was chosen.

According to \eref{eq:phicond}, in the case of $D$-optimal design, the $x_k$ that minimizes $\phi(x_k, \xi_k)$ will maximize $d(x_k, \xi_k)$. Knowing $f^T(x)$ and thus $M$, one can analytically optimize $|M|$, however that route is only possible in the simplest cases. For most models a numerical optimization routine is needed. Most numerical methods are based on points added to, or removed from a candidate design measure. One of the simplest, frequently used sequential method to generate $\xi^*$ in practice is the following:
\begin{enumerate}
\item choose $n$ arbitrary starting points that have a non-singular information matrix, thus defining $\xi_n$ with equal weights.
\item calculate $x_{n+1} = \mathrm{argmax}\left\{d(x, \xi_n)\right\}$ for $x \in \mathcal{X}$ and add it to the design.
\item repeat until algorithm converges, that is when $\max_{x} d(x, \xi_{n}) \approx p$.
\item throw away non-optimal starting points
\item the distribution of $x_i$ will suggest the of the optimal measure $\xi^*$
\end{enumerate}
There are other numerical methods finding $\xi^*$, however in practice the one above is the most straightforward to implement and interpret.

Regardless of the optimization method, the provisional $\xi^*$ should be verified by checking that $d(x, \xi_{n}) \lesssim p$ over the whole $\mathcal{X}$. Also, it can be shown that if $\mathcal{X} = \mathbb{R}^m$ then $\xi^*$ has $p$ support points with equal weights. If $\mathcal{X} \subsetneq \mathbb{R}^m$ the number of support points $n \geq p$ and those might have unequal weights.

The relative efficiency of two $D$-optimal designs $\xi_1$ and $\xi_2$ is defined as
\begin{equation}
D_\mathrm{eff} = \left\{\frac{|M(\xi_1)|}{|M(\xi_2)|}\right\}^{1/p}
\end{equation}
which is an efficiency measure independent of the $p$ dimension of the model. Because of the additivity of the information matrix, the meaning of the relative efficiency is that to provide the same tightness of model parameter estimates, $\xi_2$ requires $D_\mathrm{eff}$ number of trials compared to $\xi_1$. For example in case of $D_\mathrm{eff} = 0.5$ an experiment using $\xi_1$ would require twice as many trials as $\xi_2$.

An advantage of the $D$-optimality is that the optimum measure $\xi^*$ does not depend on the scale of the explanatory variables, linear transformations leave the $D$-optimum design unchanged.


\subsection{Optimize for a subset of parameters}

In the case when one wants to estimate an $s < p$ subset of model parameters more precisely than rest, the $D$-optimal design can be modified into a so-called $D_s$-optimal version. The information matrix can be partitioned as
\begin{equation}
M(\xi) = \left[
  \begin{array}{l l}
    M_{11}(\xi)   & M_{12}(\xi)\\
    M^T_{12}(\xi) & M_{22}(\xi)
  \end{array} \right]
\end{equation}
where $M_{11}(\xi)$ is the $s \times s$ upper sub-matrix of the information matrix. Then the $M^{11}(\xi)$, the $s \times s$ upper sub-matrix of the inverse will be
\begin{equation}
M^{11}(\xi) = \left\{M_{11}(\xi) - M_{12}(\xi)M^{-1}_{22}(\xi)M^T_{12}(\xi)\right\}^{-1}
\end{equation}
and the $D_s$ design optimizes for 
\begin{equation}
|M_{11}(\xi) - M_{12}(\xi)M^{-1}_{22}(\xi)M^T_{12}(\xi)| = \frac{|M(\xi)|}{|M_{22}(\xi)|}.
\end{equation}
This expression gives the standardized variance as
\begin{equation}
d_s(x, \xi) = f^T(x) M^{-1}(\xi)f(x) - f_2^T(x) M_{22}^{-1}(\xi)f_2(x)
\label{eq:dsvar}
\end{equation}
with optimality requirement of
\begin{equation}
d_s(x, \xi^*) \leq s
\end{equation}
and efficiency
\begin{equation}
D_\mathrm{eff} = \left\{\frac{|M(\xi_1)|/|M_{22}(\xi_1)|}{|M(\xi_2)|/|M_{22}(\xi_2)|}\right\}^{1/s}
\end{equation}

A potential difficulty with $D_s$-optimality, that $M(\xi^*)$ can turn out to be singular. In that case numerical regularization can avoid the problem, that is adding a small multiple of the identity matrix as
\begin{equation}
M_\epsilon(\xi) = M(\xi)  + \epsilon I
\end{equation}  
where $\epsilon$ is small, but large enough to be able to invert $M_\epsilon$ (in practice of the order of $10^{-5}$).

\subsection{Non-linear models and robustness}

Non-linear models cannot be expressed in the previous matrix formalism, but can be linearised to be able to use optimal design methods. If $\eta(x, \theta)$ is a non-linear function of $x$ with parameter $\theta$, Taylor expansion at the parameter estimate yields
\begin{equation}
E[Y] = \eta(x, \theta) = \eta(x, \hat \theta) + (\theta - \hat \theta) \frac{\partial \eta}{\partial \theta}(x, \theta)|(\theta = \hat \theta) + \ldots
\end{equation}
which is rewritten to emphasise the linear model as
\begin{equation}
E[Y] - \eta(x, \hat \theta) = \beta f(x)
\end{equation}
where $\beta = \theta - \hat \theta$. It is straightforward to extend the linearization to vector-parameters and to higher order Taylor-terms if needed.

A complication with non-linear models is that many times the information matrix $F(x)$ will be also a function of the parameter estimate $\hat \theta$, thus the optimal design $\xi$ is truly optimal only for a given set of parameters. One then have to evaluate the robustness of the parameter estimation for a given model and $\xi$ by calculating the change in $D_\mathrm{eff}$ as a function of the misspecified model parameters.

If more robustness is required, we can still follow the optimal design ideas to to keep some of the efficiency gains. Consider the delta function expression of $\xi$, given in \eref{eq:xidelta}. The delta function can be considered as a limiting version of a finite distribution, such as
\begin{equation}
\delta(x) = \lim_{\epsilon \rightarrow 0} \frac{1}{\sqrt{2 \pi} \epsilon } \exp\left(-\frac{x^2}{2 \epsilon^2}\right).
\label{eq:deltafinite}
\end{equation}
Using a finite $\epsilon$ in the measure $\xi$ while keeping the original support points can improve robustness. 

Under $D_s$-optimality the number of support points $n$ can be smaller than the $p$ parameters of the full model (the only requirement is $s \leq n$). In that case the optimal design is likely to be not robust, i.e. the estimates of the parameters can have large variations if the $\hat \theta$ is misspecified. Some robustness can be achieved at the expence of efficiency by reintroducing support points with small weights near the support points that are present in the $D$-optimal design but missing in the $D_s$-optimal.

\Sref{seq:exopt} and \sref{seq:exopts} contains examples of tests of robustness.

\subsection{Non-uniform error variance}

Some physical measurements are known to have non-uniform error variance as a function of the response variable $y$, e.g. photon counting which has a Poissonian distribution, thus the variance of $y_i$ is approximately $\sqrt{y_i}$ instead of a independent of $y_i$. In a general case let the variance of an observation at $x_i$ be $\sigma^2/w(x_i)$ where $w(x_i)$ are a set of known weights. If $W = \mathrm{diag}\{w(x_i)\}$, then the weighted least squares estimate of $\beta$ is
\begin{equation}
\hat \beta = (F^T W F)^{-1} F^T W y
\end{equation}
with variance
\begin{equation}
\mathrm{var}(\hat \beta) = \sigma^2 (F^T W F)^{-1}.
\end{equation}
The information matrix of \eref{eq:infcont} can be rewritten as
\begin{equation}
M(\xi) = \int_{\mathcal{X}} w(x) f(x)f^T(x) \xi(dx)
\end{equation}
and the previous methods for the $D$-optimal design can be used.

\subsection{Optimal model discrimination}
\label{sec:modeldiscrimination}
Optimal design methods can also be used to discriminate between competing models. Let's write the true model as $\eta_t(x)$. If a second model is fitted to the data, in absence of errors, its parameter estimates will be given by
\begin{equation}
\hat \theta_2(\xi) = \min_{\theta_2} \int_{\mathcal{X}} \left\{\eta_t(x) - \eta_2(x, \theta_2)\right\}^2 \xi(\mathrm{d}x)
\end{equation}
and the residual sum of squares (RSS) as
\begin{equation}
\Delta_2(\xi) = \int_{\mathcal{X}} \left\{\eta_t(x) - \eta_2(x, \hat \theta_2(\xi))\right\}^2 \xi(\mathrm{d}x).
\label{eq:rss}
\end{equation}
For high value of RSS more significant discrimination is possible between the models with the smallest number of trials. The design that maximises $\Delta_2(\xi)$ is called $T$-optimal (as ``test''). To generate the $T$-optimal $\xi^*$ we can use the General Equivalence Theorem laid out in \sref{sec:get}. The maximization $\Delta_2(\xi)$ is done by the help of its derivative
\begin{equation}
\psi_2(x, \xi) = \left\{\eta_t(x) - \eta_2(x, \hat \theta_2(\xi))\right\}^2.
\end{equation}
Based on the equivalent criteria in \sref{sec:get}, for the $T$-optimal $\xi^*$ it is true that $\psi_2(x, \xi^*) \leq \Delta_2(\xi^*)$ for $x \in \mathcal{X}$, with equivalence at the support points. For any non-optimal design, that is $\Delta_2(\xi) < \Delta_2(\xi^*)$, it will also be $\sup_{x \in \mathcal{X}} \psi_2(x, \xi) > \Delta_2(\xi^*)$.

We can then follow a very similar route to the sequential design in \sref{sec:findoptimal} to generate $\xi^*$ for any two models $\eta_1(x, \theta_1)$ and $\eta_2(x, \theta_2)$. Without a loss of generality, we can assume that the true model is $\eta_1$. Then follow:
\begin{enumerate}[(a)]
\item if there are $p$ different parameters altogether in the two models, choose $n = p+1$ separate points in the design region ($\xi_n$)
\item calculate the function value of $\eta_1$ for these points and true parameter $\theta_1$, then get the best estimate $\hat \theta_2$ of the other function
\item find $x_{n+1} = \mathrm{argmax}_{x \in \mathcal{X}}\psi_2(x, \xi_n) = \mathrm{argmax}_{x \in \mathcal{X}} \left\{\eta_1(x, \theta_1) - \eta_2(x, \hat \theta_2(\xi_n))\right\}^2$ and add to the design
\item repeat until algorithm converges in $\Delta_2(\xi_n)$, remove the non-optimal starting points and the remainder should suggest $\xi^*$.
\end{enumerate}

In practice, however, one seldom knows in advance which model will be correct. The the algorithm can be modified. Take a $k$ number of measurements and obtain parameter estimates of $\hat \theta_{1k}$ and $\hat \theta_{2k}$. The derivative function is then $\psi_2(x, \xi_k) = \left\{\eta_1(x, \theta_1) - \eta_2(x, \hat \theta_2(\xi_n))\right\}^2$. Find $x_{k+1}$ for which $\psi(x_{k+1}, \xi_k) = \sup_{x \in \mathcal{X}}\psi_2(x, \xi_n)$. The $(k+1)$th observation is than taken at $x_{k+1}$. As $k \rightarrow \infty$ one of the models converge to the true model, then this method will converge to the $T$-optimal design. Repeat the procedure until suitable precision is achieved.

When conducting experiments, if one has a theoretical estimate of $\Delta_2(\xi^*)$ and an (experimental) estimate error variance $\sigma^2$, it is possible to get an estimate of the $N$ required number of trials for discrimination at a pre-set precision. If we include the error variance in the \eref{eq:rss} expression of RSS we get
\begin{equation}
\Delta_2(\xi^*, \sigma) = \int_{\mathcal{X}} \left[\left\{\eta_t(x) - \eta_2(x, \hat \theta_2(\xi^*))\right\}^2 + \sigma^2\right]\xi(\mathrm{d}x).
\label{eq:rssvar}
\end{equation}
For linear models and large $N$ the RSS will have a non-central $\chi^2$ distribution, with non-centrality parameter
\begin{equation}
\frac{N\Delta_2(\xi^*, \sigma)}{\sigma^2 } = \frac{N \Delta_2(\xi^*)}{\sigma^2} + N
\end{equation}
and degrees of freedom of $df = N - p - 1$. Then using $\chi^2$ tables or built in functions in most mathematical software package, one can find $N$ that will have better than a pre-set $\alpha$ probability that the $\chi^2$ distribution is non-central (i.e. the given model can be rejected).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Practice

\section{Example: Lorentzian spectral line}

\subsection{3-parameter unrestricted model}
\label{seq:exopt}

The following examples of optimal design are based on spectroscopic experiments, but should be general enough to provide guidance for any other models.

In this hypothetical experiment we are trying to measure an atomic fluorescence lineshape of simple system, a Lorentzian function of the excitation light frequency:
\begin{equation}
\eta(x) = I \frac{\Gamma}{(x - x_0)^2 + \Gamma^2}
\label{eq:lorentz3}
\end{equation}
where $I$ is the height of the peak, $x_0$ is the centre frequency and $\Gamma$ is the half-with at half maximum. The partial derivatives for the Taylor-expanson are
\begin{eqnarray}
\frac{\partial \eta(x)}{\partial x_0} &=& I \frac{2 \Gamma (x - x_0)}{\left[(x - x_0)^2 + \Gamma^2\right]^2} \\
\frac{\partial \eta(x)}{\partial \Gamma} &=& I \frac{(x - x_0)^2 - \Gamma^2}{\left[(x - x_0)^2 + \Gamma^2\right]^2} \\
\frac{\partial \eta(x)}{\partial I} &=& \frac{\Gamma}{(x - x_0)^2 + \Gamma^2}
\end{eqnarray}
from which, our linearized model will be
\begin{equation}
E[y] = \eta(x) + (x_0 - \hat x_0) \frac{\partial \eta(x)}{\partial x_0} + (\Gamma - \hat \Gamma) \frac{\partial \eta(x)}{\partial \Gamma} + (I - \hat I) \frac{\partial \eta(x)}{\partial I}
\end{equation}
that can be reorganized as
\begin{eqnarray}
E[y] - \eta(x) &=& (x_0 - \hat x_0) \frac{\partial \eta(x)}{\partial x_0} + (\Gamma - \hat \Gamma) \frac{\partial \eta(x)}{\partial \Gamma} + (I - \hat I) \frac{\partial \eta(x)}{\partial I}  \nonumber \\
E[Y] &=& \beta F
\end{eqnarray}
where the coefficients are
\begin{equation}
\beta = \left[
  \begin{array}{l}
    x_0 - \hat x_0 \\
    \Gamma - \hat \Gamma\\
    I - \hat I \\
  \end{array} \right ]
\end{equation}
 and the $i$th row of $F$ is
\begin{equation}
 f^T(x_i) = \left[
  \begin{array}{l l l}
   \frac{\partial \eta(x_i)}{\partial x_0} & \frac{\partial \eta(x_i)}{\partial \Gamma} & \frac{\partial \eta(x_i)}{\partial I}
  \end{array} \right ]
\end{equation}
Without loss of generalization we can set $\hat x_0 = 0$, $\hat \Gamma = 1$, $\hat I = 1$, since the $x$ and $y$ range can always be rescaled to these values. Then we can look for the optimal design in the form of
\begin{equation}
\xi = \left\{
  \begin{array}{c c c}
    -a & 0 & a\\
    \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
  \end{array} \right \}
\end{equation}
since our original function is symmetric with respect to 0 (hence $-a$ and $a$) and $\mathcal{X}~=~\mathbb{R}$ (thus the number of design points equal the dimension of $\beta$, with equal weights). Under these considetations we can calculate the information matrix $M$ as in \eref{eq:infcont}. The maximum of the determinat is found by differentiating with respect to $a$ and finding the roots. The calculation is straightforward but lengthy, thus we only quote the result of $a~\approx~0.7746$. This analytical method can cumbersome except for the simplest models (even with the available symbolic mathematic software). On the other hand, the sequential method of finding the correct parameters is much less intensive computationally. Let our initial guess be $x = [-1, 0, 1]$, which is symmetric and equal probability, though any other $n \geq 3$ number for which $M$ is not singular could be chosen as a starting point. Then the application of the sequential algorithm will create the following set of design points:
\begin{equation}
    -1, 0, 1, 0.577, -0.577, 0, -0.789,\ldots,0,-0.775,0.775,\ldots
\label{eq:l3seq}
\end{equation}
where the final ``$\ldots$'' represents the repeatition of the $0,-0.775,0.775$ sequence. We can then ignore the starting values and conclude that to $10^-3$ precision the $D$-optimal design should be
\begin{equation}
\xi = \left\{
  \begin{array}{c c c}
    -0.775 & 0 & 0.775\\
    \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
  \end{array} \right \}
\label{eq:lorentzxi1}
\end{equation}
In the case when $\mathcal{X}$ is restricted it is possible that we have more than $p$ number of support for our final design and they won't have equal weights, thus a frequency analysis of the output of the sequential design algorithm is needed.
If we want to compare the the efficiency of the usual uniformly distributed $x$ values to the design of \eref{eq:lorentzxi1} for the same number of measurements, we have to choose a limited region for the uniform distribution. With $\mathcal{X} = [-2, 2]$ we have $D_\mathrm{eff} = 0.755$ (thus the optimal design would need $\approx 25\%$ less measurements for the same level of accuracy), while with $\mathcal{X} =[-3, 3]$ (which is still a sensible range for a uniformly distributed measurement) it is $D_\mathrm{eff} = 0.548$.

\subsection{Optimizing for individual parameters}
\label{seq:exopts}

If we optimize the model \eref{eq:lorentz3} for the measurement of the centre position $x_0$, the optimal measure becomes
\begin{equation}
\xi^*_{x_0} = \left\{
  \begin{array}{c c}
    -0.576 & 0.576 \\
    \frac{1}{2} & \frac{1}{2}\\
  \end{array} \right \}
\end{equation}
which has fewer number of points than the original model, thus while the centre is obviously measurable, the width and hight is not observable anymore.  On $\mathcal{X} = [-2,2]$ the efficiency is $D_\mathrm{eff} = 0.449$.

This is however not very robust. Can increase robustness with finite-$\sigma$ design, or based on comparison with the $D$-optimal design (i.e. inserting a point at $x=0$).
\begin{equation}
\xi^*_{x_0} = \left\{
  \begin{array}{c c c}
    -0.576 & 0 & 0.576 \\
    \frac{1}{2}-\epsilon/2 & \epsilon & \frac{1}{2}-epsilon/2\\
  \end{array} \right \}
\label{eq:dsx0robust}
\end{equation}


Optimization for the with yields
\begin{equation}
\xi^*_{\Gamma} = \left\{
  \begin{array}{c c c}
    -1.188 & 0 &  1.188 \\
    0.353 & 0.294 & 0.354\\
  \end{array} \right \}
\end{equation}
which has necessarily 3-point support for $\Gamma$ to be observable, though with reduced weight for the middle. On $\mathcal{X} = [-2,2]$ the efficiency is $D_\mathrm{eff} = 0.609$.


The optimal measurement of the $I$ height is again different and highlights the need for validating the final design buy calculating $d(x, \xi)$ over the design region. If one just follows the sequential method, calculating a number of measurement point, then throwing away a fraction of the initial ones as ``not-yet converged'' region, the final design measure would be
\begin{equation}
\xi_{I} = \left\{
  \begin{array}{c c}
    -1 &  1 \\
    0.5 & 0.5\\
  \end{array} \right \}
\end{equation}
Clearly with this measure the height is not observable, as there are an infinite number of curves with different $\Gamma$ and $I$ going threw them. Checking the standardized variance $\max d(x, \xi) = 4$, which is larger than the number of parameters $p = 1$ in this case. The maximum occurs at $x = 0$, giving a clue, that we have to have at least one measurement at the centre. Including even one more measurement there will bring $\max d(x, \xi) \approx 1$, signalling the approximate optimality of this design. Thus, the final measure can be written as
\begin{equation}
\xi^*_{I} = \left\{
  \begin{array}{c c c}
    -1 & 0 &  1 \\
    0.5-\epsilon/2 & \epsilon & 0.5-\epsilon/2\\
  \end{array} \right \}
\end{equation}
where $\epsilon$ has to be chosen so that $N \epsilon \geq 1$ for the $N$ measurements. On $\mathcal{X} = [-2,2]$ the efficiency is $D_\mathrm{eff} = 0.579$.

\subsection{Restricted design region}

The design region can be restricted for any reason. For example in ion trapping experiments generally the frequency is confined to $x < x_0$ as tuning to the positive side of the centre will result in completely different model of interaction. Let's restrict our region to $\mathcal{X} = [-\infty, 0]$ (where $x_0 = 0$ is implicit), the sequential algorithm gives
\begin{equation}
\xi^* = \left\{
  \begin{array}{c c c}
    -1.285 & -0.395 &  0 \\
     \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
  \end{array} \right \}
\end{equation}
On $\mathcal{X} = [-2,0]$ the efficiency is $D_\mathrm{eff} = 0.622$. On the restricted region the optimal design has higher efficiency gain compared the uniformly distributed measurements than when the region is not restricted.

\subsection{4-parameter, restricted model}

Our spectroscopy measurement might include a background (or couldn't substract a known value from our measurements), thus the model of \eref{eq:lorentz3} needs to be extended:
\begin{equation}
\eta(x) = I \frac{\Gamma}{(x - x_0)^2 + \Gamma^2} + \eta_0
\label{eq:lorentz4}
\end{equation}
The corresponding term in the design matrix $f$ will be
\begin{equation}
\frac{\partial \eta(x)}{\partial \eta_0} = 1
\end{equation}
which is independent of $x$. This has the consequence, that we can no longer have unrestricted $\mathcal{X}$, as at $x~\rightarrow~\infty$ we also have $d(x,\xi)~\rightarrow~\infty$. Also, the optimal design $\xi$ will be independent of the estimate of $\hat n$ (which is not true for the other parameter estimates). Our design matrix has rows of
\begin{equation}
 f^T(x_i) = \left[
  \begin{array}{l l l l}
   \frac{\partial \eta(x_i)}{\partial x_0} & \frac{\partial \eta(x_i)}{\partial \Gamma} & \frac{\partial \eta(x_i)}{\partial I} & \frac{\partial \eta(x_i)}{\partial \eta_0}
  \end{array} \right ]
\end{equation}
For design region $\mathcal{X} = [-2, 2]$ the sequential algorithm results in design measure of
\begin{equation}
\xi^* = \left\{
  \begin{array}{c c c c c}
    -2 & -0.716 & 0 & 0.716 & 2\\
    \frac{1}{8} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{8}\\
  \end{array} \right \}
\end{equation}
whic has now a higher number of support points than control parameters $p$. When checking this measure for $D$-optimality, however, the equality $d(x, \xi) = p$ still holds at all support points. Compared to the uniform distribution on the same interval, $D_\mathrm{eff} = 0.728$. If the design region is extended to $\mathcal{X} = [-3, 3]$ the optimal measure becomes
\begin{equation}
\xi^* = \left\{
  \begin{array}{c c c c c}
    -3 & -0.748 & 0 & 0.748 & 3\\
    \frac{1}{8} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{8}\\
  \end{array} \right \}
\end{equation}
with $D_\mathrm{eff} = 0.641$.

\subsection{Discriminating Lorentzian and Gaussian}

As an example for $T-optimal$ design for model selection, let's try to discriminate between a Lorentzian response (with the functional form of \eref{eq:lorentz3}) and a Gaussian function,
\begin{equation}
\eta_G(x) = \frac{A}{\sigma} \exp\left(-\frac{(x-x_0)^2}{2 \sigma^2}\right)
\label{eq:gauss}
\end{equation}
where $A$ is the height, $\sigma$ is the half-width, and $x_0$ is the centre position. In this setting the two models differ in 4 parameters ($I$ and $\Gamma$ for the Lorentzian function, $A$ and $\sigma$ for the Gaussian), suggesting that the optimal design will have $4+1=5$ point support. For the example assume that the true distribution is a Lorentzian with $x_0 = 0$, $\Gamma = 1$  and $I = 1$. From a starting series of $[-2, -1, 0, 1, 2]$ the sequential algorithm results in a measure
\begin{equation}
\xi^* \approx \left\{
  \begin{array}{c c c c c}
    -2.74 & -0.96 & 0 & 0.96 & 2.73\\
    0.301 & 0.126 & 0.146 & 0.126 & 0.301\\
  \end{array} \right \}
\label{eq:lorgauss}
\end{equation}
where the approximate equivalence means that the optimal measure is seen to be very sensitive to the support points and their weights, and \eref{eq:lorgauss} should be close as close to the optimal solution as it is possible with limited $N$ trials. \Fref{fig:lorgauss} shows how the Lorentzian, the best fitting Gaussian and the support points of \eref{eq:lorgauss}.


Optimal $\Delta_2(\xi^*) \approx 7.3 \times 10^{-3}$

\section{What have we learned?}

It is possible to optimize and in the same time simplify the experiment but care has to be taken. Not for $D$-optimality but for other types the design has to be recalculated - though it is surprisingly resiliant to change (given the fact that there are not too many different points under measurement). Also, the verification of the model is essential. All can be automated, though. Depending on the model one can achieve significant gains in efficiency, especially when optimizing for a subset of parameters. This method does augment the normal experimental procedures, but does not replace them. The systematic effects (e.g. parameter drift) could have more complicated effect on the model, thus if suspect of such effects taking place, mapping time onto the fited parameters could be not completely straightforward


\ack Thanks for the dudes and to the National Science Council of Taiwan.


\begin{figure}
\includegraphics[width=0.9\textwidth]{figure1.eps}
\caption{The standardized variance $d(x, \xi^*)$ at the $D$-optimal design for a Lorentzian function with parameters $x_0=0$, $\Gamma = 1$, $A = 1$, see \sref{seq:exopt}. The dashed line is equal to the $p$ number of parameters of the model which is the limit for $d(x, \xi)$ in case of $D$-optimality. The support points are when the two lines intersect.}
\label{fig:lorentz3d}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure2.eps}
\caption{Least-square fitted variance of the parameter $x_0$ for a Lorentzian with parameters $x_0=0$, $\Gamma = 1$, $A = 1$, added error with $\sigma=0.1$, and $n=201$ points distributed as: 1) uniformly over the [-3, 3] interval, 2) according to the $D$-optimal design of {eq:lorentzxi1}, 3) $D_s$-optimal design for $x_0$ with $\epsilon = 0.025$ in \eref{eq:dsx0robust}.}
\label{fig:simvariance}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure3.eps}
\caption{Efficiency of the sequential algorithm shown in \eref{eq:l3seq} as a function of the $N$ number of steps, compared to the continuous optimal design of \eref{eq:lorentzxi1}.}
\label{fig:xinefficiency}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure4.eps}
\caption{Robustness of the $D$-optimal design \eref{eq:lorentzxi1}. The efficiency is compared to the uniform distribution over the interval $x\in [-2,2]$ as a function $\Delta x_0$ misspecification of the centre position. The shown sub-optimal design is generated using \eref{eq:deltafinite} with $\epsilon = 0.4$.}
\label{fig:x0robust}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure5.eps}
\caption{Robustness of the $D$-optimal design \eref{eq:lorentzxi1}. The efficiency is compared to the uniform distribution over the interval $x\in [-2,2]$ as a function $\Gamma'$ true linewidth when the optimal design is generated with $\hat \Gamma = 1$. The sub-optimal design used $\epsilon = 0.4$.}
\label{fig:grobust}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure6.eps}
\caption{Robustness of the $D_s$-optimal design for $x_0$, shown in \eref{eq:dsx0robust}, using $\epsilon = 0.01$. The efficiency is compared to the uniform distribution over the interval $x\in [-2,2]$ as a function $\Delta x_0$ misspecification of the centre position. The sub-optimal design used $\epsilon = 0.15$. The horizontal scale is different from \fref{fig:x0robust}.}
\label{fig:dsx0robust}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure7.eps}
\caption{Lorentzian ($x_0 = 0$, $\Gamma = 1$, $I = 1$, continuous line), best fitting Gaussian ($x_0 = 0$, $\sigma = 1.063$, $A = 0.971$, dashed line) and the $T$-optimal support points of \eref{eq:lorgauss} (vertical lines).}
\label{fig:lorgauss}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure8.eps}
\caption{The derivative function $\psi(x, \xi_N)$ of the $T$-optimal design for discriminating a Lorentzian function from a Gaussian in the same setting as in \fref{fig:lorgauss}. The crosses show the support points generated by the sequential algorithm, with the first 20 (non-optimal, converging points) thrown away. Dashed line is the $\Delta_2(\xi^*)$ residual sum of squares at the optimal design.}
\label{fig:topt}
\end{figure}

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure9.eps}
\caption{Convergence of the sequential algorithm generating the support points of a $T$-design for discriminating between a Lorentzian and a Gaussian in the same setting as in \fref{fig:lorgauss}. $\xi_N$ is the design function after $N$ steps, $\xi^*$ is the T-optimal design in \eref{eq:lorgauss}.}
\end{figure}


\section*{References}
\bibliographystyle{unsrt}
\bibliography{optimal}
\end{document}
