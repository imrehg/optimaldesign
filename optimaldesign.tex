\documentclass[12pt]{iopart}
\usepackage{graphicx}
\usepackage{iopams}
\begin{document}
\article[Optimal design for spectroscopy]{TECHNICAL DESIGN NOTE}{Optimal design for spectroscopy}
%%\author{G Imreh and W-Y Cheng}
\address{Institute of Atomic and Molecular Sciences, Academia Sinica, Taiwan}
%%\ead{\mailto{imrehg@gmail.com}}
\begin{abstract}
Optimal design methodology for spectroscopy and other fitted things.
\end{abstract}

\noindent{\it Keywords\/}:

\pacs{}
%%\submitto{\MST}

\section{Introduction}

Experiments are not limited to the physical sciences, but many areas of science, even industry makes use of them: a process is described by a mathematical model and repeated observations are used to evaluate the model and estimate its free parameters. When designing an experiment, the most common aims are the ability to estimate the model parameters without bias and as well as possible, ability to detect lack of fit, and to use the fewest number of observations \cite{Box1975,Box1987}. The last aim is often considered secondary in physical experiments, however as this note will show, the three aims are interconnected.

The possible input parameters for an experiment form the $\mathcal{X}$ design region. From $\mathcal{X}$ one chooses $N$ points $x_i$, $i = 1, \ldots, N$ to conduct the experiment. Using optimal experimental design (OED) theory, with different choices of $x_i$ one can adjust the experiment towards different goals: overall best estimate of the model parameters, best estimate (smallest variance) of a subset of important parameters at the expence of others, best estimate of a function of the model parameters, most sensitive lack-of-fit measure. The list is by no means exhausive, but represent the most common cases in physics experiments. Most researchers spread the observations uniformly in the region of interest, which is often sub-optimal in all criteria. OED is a rich area of statistics, mostly used in medical research and industrial quality control, mainly due to those fields close connection to statistics, not because of the lack relevance to other fields. There were previous efforts to use OED for physics, such as optimal measurement of processes described by differential equations, e.g. the placement of temperature sensors in heat conductivity experiments \cite{Emery1998}. This note hopes to introduce the methodology of OED for model parameter estimation (fitting), examine its suitability for physical experiments and give examples to help implementation.


\section{Optimal design}
Short introduction of linear regression using matrix algebra, only to show the notation in this paper. For more detailed explanation check any standard linear algebra text book. Measure $y_i$ at $x_i$ points ($i=1, \ldots, N$)
\begin{equation}
E[Y] = F \beta
\end{equation}
where $Y$ is the $N \times 1$ vector of responses, $\beta$ is a $p \times 1$ vector of unknown parameters, and $F$ is the $N \times p$ extended design matrix. The $i$th row of $F$ is $f^T(x_i)$, a known function of the $m$ explanatory variables.

Least squares regression is then:
\begin{equation}
\hat \beta = (F^T F)^{-1} F^T y
\end{equation}
The covariance matrix of the least squares estimates is
\begin{equation}
\mathrm{var} \hat \beta = \sigma^2 (F^T F)^{-1}
\end{equation}


\subsection{Obtimal design}
Over $\xi$ is a continuous measure over $\mathcal{X}$. If $\xi$ has $n$ distinct points, then
\begin{equation}
\xi = \left\{
  \begin{array}{l l l l}
    x_1 & x_2 & \ldots & x_n\\
    w_1 & w_2 & \ldots & w_n\\
  \end{array} \right \}
\end{equation}
where $w_i$ is the weight of point $x_i$, $\int_{\mathcal{X}}\xi(dx) = 1$ and $0 \leq w_i \leq 1$ for every $i$.

Exact design is for realizable integers for a given $N$.
\begin{equation}
\xi_N = \left\{
  \begin{array}{l l l l}
    x_1 & x_2 & \ldots & x_n\\
    r_1/N & r_2/N & \ldots & r_n/N\\
  \end{array} \right \}
\end{equation}

The N-trial design:
\begin{equation}
F^T F = \sum_{i=1}^N f(x_i) f(x_i)^T
\end{equation}.
The information matrix is
\begin{equation}
M(\xi) = \int_{\mathcal{X}} f(x)f^T(x) \xi(dx) = \sum_{i=1}^n f(x_i)f^T(x_i)w_i
\label{eq:infcont}
\end{equation}
summing over the $n$ design points.
For the exact design we have to scale as
\begin{equation}
M(\xi_N) = \frac{F^T F}{N}
\end{equation}
The variance of the response is given by
\begin{equation}
\mathrm{var}\{\hat y(x)\} = \sigma^2 f^T(x) M^{-1}(\xi) f(x)
\end{equation}
from where the standardized variance of the predicted response is
\begin{equation}
d(x, \xi) = f^T(x) M^{-1}(\xi) f(x)
\end{equation}
that is function of both $x$ where the prediction is made and the design $\xi$.

D-optimal design (as "determinant"): $|F^T F|$ is maximal.
One practical testing method whether one found the D-optimal design: for D-optimality, $d(x, \xi) \leq p$, with equality at the design points.
Works for any differentiable functions, even if only differentiable numerically.

If $\mathcal{X} = \mathbb{R}$ then $\xi$ has $p$ support points, and all of them have equal weight. If $\mathcal{X} \subsetneq \mathbb{R}$ then $n \geq p$, and might have unequal weights.

Talk about the effect of singular matrices.

There is no standard way to caulculate the optimal $\xi$ in general. There are methods developed (see citation, ...). One subset of methods is sequential design: adding points one-by-one, or removing points one-by-one. One practical method is outlined here:

\begin{enumerate}
\item Nonsingular start with $n$ points
\item Calculate argmax $d(x, \xi_n)$ for $x \in \mathcal{X}$,
\item Add that point to the bunch as $n+1$
\item Repeat until algorithm converges
\item Can throw away starting points
\item Should have an approximate measure of the optimal $\xi^*$
\end{enumerate}
Works for any differentiable functions, even if only differentiable numerically.

\subsection{Restricted optimal design}

\begin{equation}
d_s(x, \xi) = f^T(x) M^{-1}(\xi)f(x) - f_2^T(x) M_{22}^{-1}(\xi)f_2(x)
\label{eq:dsvar}
\end{equation}

\subsection{Sub-obtimal design for robustness}

Surprisingly robust, the $\hat \beta$ has to be very far from the true to lose power... Do simulation!

\subsection{Finite sample size}

The relative D-efficiency of two models $\xi_1$ and $x_2$ is defined as
\begin{equation}
D_\mathrm{eff} = \left\{\frac{M(\xi_1)}{M(\xi_2)}\right\}^{1/p}
\end{equation}
which results in an efficiency measure which is proportional to the design size, irrespective to the dimension of the model. Thus e.g. $D_\mathrm{eff} = 0.5$ means that $1 / D_\mathrm{eff} = 2$ replicates of design measure $\xi_1$ would be as efficient as one replicate of $\xi_2$.

In this paper, thus we can set $\xi_1$ to the uniform distribution of measurement points, and compare optimized versions of $\xi_2$ to that.

\subsection{Design for restricted parameter space}

Doing the same method for restricting $\mathcal{X}$ is just as possible.

\subsection{Heterosadicity solved by GLM}
\subsection{Model validation}

Maximize the lack-of fit parameter: T-optimization (as "test"). If there are $p_1$ and $p_2$

\subsection{Imprecision of $\hat \beta$}

How to calculate the predicted effect of imprecise parameters?

\subsection{Functions of variables}

C-optimum design (as "constans"):

\begin{equation}
\mathrm{var}\left\{g(\hat \theta)\right\} = \mathrm{var}(c^T \hat \beta) = c^T M^{-1}(\xi) c
\end{equation}
where $c$ is a $p \times 1$ vector of known constans. Generally, it can be defined from the Tailor expansion:
\begin{equation}
c_i(\theta) = \frac{\partial g(\theta)}{\partial \theta_i}
\end{equation}

Have to be careful, though, because the model can be singular. For linear models this means maybe only linear combinations of parameters can be estimated, for nonlinear models it is possible that the parameter won't be observable. Care needed!

\section{Example: Lorentzian spectral line}

\subsection{3-parameter unrestricted model}

The following examples of optimal design are based on spectroscopic experiments, but should be general enough to provide guidance for any other models.

In this hypothetical experiment we are trying to measure an atomic fluorescence lineshape of simple system, a Lorentzian function of the excitation light frequency:
\begin{equation}
\eta(x) = I \frac{\Gamma}{(x - x_0)^2 + \Gamma^2}
\label{eq:lorentz3}
\end{equation}
where $I$ is the height of the peak, $x_0$ is the centre frequency and $\Gamma$ is the half-with at half maximum. The partial derivatives for the Taylor-expanson are
\begin{eqnarray}
\frac{\partial \eta(x)}{\partial x_0} &=& I \frac{2 \Gamma (x - x_0)}{\left[(x - x_0)^2 + \Gamma^2\right]^2} \\
\frac{\partial \eta(x)}{\partial \Gamma} &=& I \frac{(x - x_0)^2 - \Gamma^2}{\left[(x - x_0)^2 + \Gamma^2\right]^2} \\
\frac{\partial \eta(x)}{\partial I} &=& \frac{\Gamma}{(x - x_0)^2 + \Gamma^2}
\end{eqnarray}
from which, our linearized model will be
\begin{equation}
E[y] = \eta(x) + (x_0 - \hat x_0) \frac{\partial \eta(x)}{\partial x_0} + (\Gamma - \hat \Gamma) \frac{\partial \eta(x)}{\partial \Gamma} + (I - \hat I) \frac{\partial \eta(x)}{\partial I}
\end{equation}
that can be reorganized as
\begin{eqnarray}
E[y] - \eta(x) &=& (x_0 - \hat x_0) \frac{\partial \eta(x)}{\partial x_0} + (\Gamma - \hat \Gamma) \frac{\partial \eta(x)}{\partial \Gamma} + (I - \hat I) \frac{\partial \eta(x)}{\partial I}  \nonumber \\
E[Y] &=& \beta F
\end{eqnarray}
where the coefficients are
\begin{equation}
\beta = \left[
  \begin{array}{l}
    x_0 - \hat x_0 \\
    \Gamma - \hat \Gamma\\
    I - \hat I \\
  \end{array} \right ]
\end{equation}
 and the $i$th row of $F$ is
\begin{equation}
 f^T(x_i) = \left[
  \begin{array}{l l l}
   \frac{\partial \eta(x_i)}{\partial x_0} & \frac{\partial \eta(x_i)}{\partial \Gamma} & \frac{\partial \eta(x_i)}{\partial I}
  \end{array} \right ]
\end{equation}
Without loss of generalization we can set $\hat x_0 = 0$, $\hat \Gamma = 1$, $\hat I = 1$, since the $x$ and $y$ range can always be rescaled to these values. Then we can look for the optimal design in the form of
\begin{equation}
\xi = \left\{
  \begin{array}{c c c}
    -a & 0 & a\\
    \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
  \end{array} \right \}
\end{equation}
since our original function is symmetric with respect to 0 (hence $-a$ and $a$) and $\mathcal{X}~=~\mathbb{R}$ (thus the number of design points equal the dimension of $\beta$, with equal weights). Under these considetations we can calculate the information matrix $M$ as in \eref{eq:infcont}. The maximum of the determinat is found by differentiating with respect to $a$ and finding the roots. The calculation is straightforward but lengthy, thus we only quote the result of $a~\approx~0.7746$. This analytical method can cumbersome except for the simplest models (even with the available symbolic mathematic software). On the other hand, the sequential method of finding the correct parameters is much less intensive computationally. Let's start with a design of $x = [-1, 0, 1]$ which is still symetric and equal probability, but this is just to speed up the convergence of the algorithm. Any other $n \geq 3$ number for which $M$ is not singular could be chosen as a starting point. Then the sequential algorithms will create the following set of design points:
\begin{equation}
    -1, 0, 1, 0.577, -0.577, 0, -0.789,\ldots,0,-0.775,0.775,\ldots
\end{equation}
where the final ``$\ldots$'' just repeats the $0,-0.775,0.775$ sequence. We can then ignore the starting values and conclude that are design should be
\begin{equation}
\xi = \left\{
  \begin{array}{c c c}
    -0.775 & 0 & 0.775\\
    \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
  \end{array} \right \}
\label{eq:lorentzxi1}
\end{equation}
In the case when $\mathcal{X}$ is restricted it is possible that we have more than $p$ number of support for our final design and they won't have equal weights, thus a frequency analysis of the output of the sequential design algorithm is needed.
If we want to compare the the efficiency of the usual uniformly distributed $x$ values to the design of \eref{eq:lorentzxi1} for the same number of measurements, we have to choose a limited region for the uniform distribution. With $\mathcal{X} = [-2, 2]$ we have $D_\mathrm{eff} = 0.755$ (thus the optimal design would need $\approx 25\%$ less measurements for the same level of accuracy), while with $\mathcal{X} =[-3, 3]$ (which is still a sensible range for a uniformly distributed measurement) it is $D_\mathrm{eff} = 0.548$.

\subsection{Optimizing for individual parameters}

If we optimize the model \eref{eq:lorentz3} for the measurement of the centre position $x_0$, the optimal measure becomes
\begin{equation}
\xi^*_{x_0} = \left\{
  \begin{array}{c c}
    -0.576 & 0.576 \\
    \frac{1}{2} & \frac{1}{2}\\
  \end{array} \right \}
\end{equation}
which has fewer number of points than the original model, thus while the centre is obviously measurable, the width and hight is not observable anymore.  On $\mathcal{X} = [-2,2]$ the efficiency is $D_\mathrm{eff} = 0.449$.

Optimization for the with yields
\begin{equation}
\xi^*_{\Gamma} = \left\{
  \begin{array}{c c c}
    -1.188 & 0 &  1.188 \\
    0.353 & 0.294 & 0.354\\
  \end{array} \right \}
\end{equation}
which has necessarily 3-point support for $\Gamma$ to be observable, though with reduced weight for the middle. On $\mathcal{X} = [-2,2]$ the efficiency is $D_\mathrm{eff} = 0.609$.


The optimal measurement of the $I$ height is again different and highlights the need for validating the final design buy calculating $d(x, \xi)$ over the design region. If one just follows the sequential method, calculating a number of measurement point, then throwing away a fraction of the initial ones as ``not-yet converged'' region, the final design measure would be
\begin{equation}
\xi_{I} = \left\{
  \begin{array}{c c}
    -1 &  1 \\
    0.5 & 0.5\\
  \end{array} \right \}
\end{equation}
Clearly with this measure the height is not observable, as there are an infinite number of curves with different $\Gamma$ and $I$ going threw them. Checking the standardized variance $\max d(x, \xi) = 4$, which is larger than the number of parameters $p = 1$ in this case. The maximum occurs at $x = 0$, giving a clue, that we have to have at least one measurement at the centre. Including even one more measurement there will bring $\max d(x, \xi) \approx 1$, signalling the approximate optimality of this design. Thus, the final measure can be written as
\begin{equation}
\xi^*_{I} = \left\{
  \begin{array}{c c c}
    -1 & 0 &  1 \\
    0.5-\epsilon/2 & \epsilon & 0.5-\epsilon/2\\
  \end{array} \right \}
\end{equation}
where $\epsilon$ has to be chosen so that $N \epsilon \geq 1$ for the $N$ measurements. On $\mathcal{X} = [-2,2]$ the efficiency is $D_\mathrm{eff} = 0.579$.

\subsection{Restricted design region}

The design region can be restricted for any reason. For example in ion trapping experiments generally the frequency is confined to $x < x_0$ as tuning to the positive side of the centre will result in completely different model of interaction. Let's restrict our region to $\mathcal{X} = [-\infty, 0]$ (where $x_0 = 0$ is implicit), the sequential algorithm gives
\begin{equation}
\xi^* = \left\{
  \begin{array}{c c c}
    -1.285 & -0.395 &  0 \\
     \frac{1}{3} & \frac{1}{3} & \frac{1}{3}\\
  \end{array} \right \}
\end{equation}
On $\mathcal{X} = [-2,0]$ the efficiency is $D_\mathrm{eff} = 0.622$. On the restricted region the optimal design has higher efficiency gain compared the uniformly distributed measurements than when the region is not restricted.

\subsection{4-parameter, restricted model}

Our spectroscopy measurement might include a background (or couldn't substract a known value from our measurements), thus the model of \eref{eq:lorentz3} needs to be extended:
\begin{equation}
\eta(x) = I \frac{\Gamma}{(x - x_0)^2 + \Gamma^2} + \eta_0
\label{eq:lorentz4}
\end{equation}
The corresponding term in the design matrix $f$ will be
\begin{equation}
\frac{\partial \eta(x)}{\partial \eta_0} = 1
\end{equation}
which is independent of $x$. This has the consequence, that we can no longer have unrestricted $\mathcal{X}$, as at $x~\rightarrow~\infty$ we also have $d(x,\xi)~\rightarrow~\infty$. Also, the optimal design $\xi$ will be independent of the estimate of $\hat n$ (which is not true for the other parameter estimates). Our design matrix has rows of
\begin{equation}
 f^T(x_i) = \left[
  \begin{array}{l l l l}
   \frac{\partial \eta(x_i)}{\partial x_0} & \frac{\partial \eta(x_i)}{\partial \Gamma} & \frac{\partial \eta(x_i)}{\partial I} & \frac{\partial \eta(x_i)}{\partial \eta_0}
  \end{array} \right ]
\end{equation}
For design region $\mathcal{X} = [-2, 2]$ the sequential algorithm results in design measure of
\begin{equation}
\xi^* = \left\{
  \begin{array}{c c c c c}
    -2 & -0.716 & 0 & 0.716 & 2\\
    \frac{1}{8} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{8}\\
  \end{array} \right \}
\end{equation}
whic has now a higher number of support points than control parameters $p$. When checking this measure for D-optimality, however, the equality $d(x, \xi) = p$ still holds at all support points. Compared to the uniform distribution on the same interval, $D_\mathrm{eff} = 0.728$. If the design region is extended to $\mathcal{X} = [-3, 3]$ the optimal measure becomes
\begin{equation}
\xi^* = \left\{
  \begin{array}{c c c c c}
    -3 & -0.748 & 0 & 0.748 & 3\\
    \frac{1}{8} & \frac{1}{4} & \frac{1}{4} & \frac{1}{4} & \frac{1}{8}\\
  \end{array} \right \}
\end{equation}
with $D_\mathrm{eff} = 0.641$.

\section{What have we learned?}

It is possible to optimize and in the same time simplify the experiment but care has to be taken. Not for D-optimality but for other types the design has to be recalculated - though it is surprisingly resiliant to change (given the fact that there are not too many different points under measurement). Also, the verification of the model is essential. All can be automated, though. Depending on the model one can achieve significant gains in efficiency, especially when optimizing for a subset of parameters. This method does augment the normal experimental procedures, but does not replace them. The systematic effects (e.g. parameter drift) could have more complicated effect on the model, thus if suspect of such effects taking place, mapping time onto the fited parameters could be not completely straightforward


\ack Thanks for the dudes and to the National Science Council of Taiwan.

\section*{References}
\bibliographystyle{unsrt}
\bibliography{optimal}
\end{document}
